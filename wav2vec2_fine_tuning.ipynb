{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_generator import DatasetGenerator\n",
    "from utils import extract_all_chars, save_dict_as_json\n",
    "from data_preprocessor import Preprocessor\n",
    "from data_augmentation import AudioAugmentation\n",
    "from data_collator import DataCollatorCTCWithPadding\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.utils.data import Dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = 'dataset'\n",
    "DATA_PATH = 'data.csv'\n",
    "\n",
    "word_character_map = {\n",
    "    'iskljuci': 'isključi',\n",
    "    'ukljuci': 'uključi'\n",
    "}\n",
    "\n",
    "TORCH_DATASETS_DIR = 'torch_datasets'\n",
    "\n",
    "MODEL_NAME = \"wav2vec2-finetuned-voice-commands\"\n",
    "MODELS_DIR = 'models'\n",
    "MODEL_LOGS_DIR = 'models/logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data.csv\n"
     ]
    }
   ],
   "source": [
    "dg = DatasetGenerator(word_character_map)\n",
    "\n",
    "\n",
    "dg.generate(input_dir=AUDIO_DIR, output_file=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_filepath</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset\\iskljuci-19-21-1.wav</td>\n",
       "      <td>isključi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset\\iskljuci-19-21-2.wav</td>\n",
       "      <td>isključi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset\\iskljuci-19-21-3.wav</td>\n",
       "      <td>isključi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset\\iskljuci-38-21-1.wav</td>\n",
       "      <td>isključi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset\\iskljuci-38-21-2.wav</td>\n",
       "      <td>isključi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 audio_filepath      text\n",
       "0  dataset\\iskljuci-19-21-1.wav  isključi\n",
       "1  dataset\\iskljuci-19-21-2.wav  isključi\n",
       "2  dataset\\iskljuci-19-21-3.wav  isključi\n",
       "3  dataset\\iskljuci-38-21-1.wav  isključi\n",
       "4  dataset\\iskljuci-38-21-2.wav  isključi"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = 'vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0,\n",
       " 'l': 1,\n",
       " 's': 2,\n",
       " 'a': 3,\n",
       " 'r': 4,\n",
       " 'k': 5,\n",
       " 'u': 6,\n",
       " 'č': 7,\n",
       " 'e': 8,\n",
       " 'z': 9,\n",
       " 'j': 10,\n",
       " 't': 11,\n",
       " 'o': 12,\n",
       " 'v': 13,\n",
       " '|': 14,\n",
       " '[UNK]': 15,\n",
       " '[PAD]': 16}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = df['text'].unique()\n",
    "\n",
    "vocab_list = extract_all_chars(words)\n",
    "\n",
    "vocab_list.extend(['|', '[UNK]', '[PAD]'])\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_as_json(VOCAB_PATH, vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the tokenizer, feature extractor and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, \n",
    "                                             sampling_rate=16000, \n",
    "                                             padding_value=0.0, \n",
    "                                             do_normalize=True, \n",
    "                                             return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = AudioAugmentation(min_noise=0, max_noise=.005, time_stretch_rate=.9, pitch_shift_n_steps=2)\n",
    "\n",
    "train_preprocessor = Preprocessor(processor=processor, sr=16000, audio_augmentation=aug, augment_count=2)\n",
    "val_preprocessor = Preprocessor(processor=processor, sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_values</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tensor(0.1956), tensor(0.3131), tensor(0.2815...</td>\n",
       "      <td>[tensor(12), tensor(11), tensor(13), tensor(12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tensor(0.4701), tensor(0.3975), tensor(0.6470...</td>\n",
       "      <td>[tensor(12), tensor(11), tensor(13), tensor(12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tensor(0.9730), tensor(0.1340), tensor(0.2596...</td>\n",
       "      <td>[tensor(12), tensor(11), tensor(13), tensor(12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[tensor(-0.0024), tensor(-0.0061), tensor(-0.0...</td>\n",
       "      <td>[tensor(13), tensor(4), tensor(3), tensor(11),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tensor(-0.0405), tensor(0.0209), tensor(0.112...</td>\n",
       "      <td>[tensor(13), tensor(4), tensor(3), tensor(11),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        input_values  \\\n",
       "0  [tensor(0.1956), tensor(0.3131), tensor(0.2815...   \n",
       "1  [tensor(0.4701), tensor(0.3975), tensor(0.6470...   \n",
       "2  [tensor(0.9730), tensor(0.1340), tensor(0.2596...   \n",
       "3  [tensor(-0.0024), tensor(-0.0061), tensor(-0.0...   \n",
       "4  [tensor(-0.0405), tensor(0.0209), tensor(0.112...   \n",
       "\n",
       "                                              labels  \n",
       "0  [tensor(12), tensor(11), tensor(13), tensor(12...  \n",
       "1  [tensor(12), tensor(11), tensor(13), tensor(12...  \n",
       "2  [tensor(12), tensor(11), tensor(13), tensor(12...  \n",
       "3  [tensor(13), tensor(4), tensor(3), tensor(11),...  \n",
       "4  [tensor(13), tensor(4), tensor(3), tensor(11),...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset into train and validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# Preprocess data\n",
    "preprocessed_train_data = []\n",
    "preprocessed_val_data = []\n",
    "\n",
    "# Preprocess training data\n",
    "for _, row in train_df.iterrows():\n",
    "    preprocessed_train_data.extend(train_preprocessor.preprocess(row))\n",
    "\n",
    "# Preprocess validation data\n",
    "for _, row in val_df.iterrows():\n",
    "    preprocessed_val_data.extend(val_preprocessor.preprocess(row))\n",
    "\n",
    "# Create new dataframes for the preprocessed data\n",
    "train_df = pd.DataFrame(preprocessed_train_data)\n",
    "val_df = pd.DataFrame(preprocessed_val_data)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch Dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_values\": self.data.iloc[idx][\"input_values\"],\n",
    "            \"labels\": self.data.iloc[idx][\"labels\"],\n",
    "        }\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = AudioDataset(train_df)\n",
    "val_dataset = AudioDataset(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TORCH_DATASETS_DIR):\n",
    "    os.mkdir(TORCH_DATASETS_DIR)\n",
    "\n",
    "torch.save(train_dataset, os.path.join(TORCH_DATASETS_DIR, 'train.pt'))\n",
    "torch.save(val_dataset, os.path.join(TORCH_DATASETS_DIR, 'val.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.2,\n",
    "    hidden_dropout=0.2,\n",
    "    feat_proj_dropout=0.05,\n",
    "    mask_time_prob=0.04,\n",
    "    layerdrop=0.15,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")\n",
    "\n",
    "model.config.vocab_size = len(processor.tokenizer)\n",
    "\n",
    "model.freeze_feature_encoder()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.mkdir(MODELS_DIR)\n",
    "\n",
    "if not os.path.exists(MODEL_LOGS_DIR):\n",
    "    os.mkdir(MODEL_LOGS_DIR)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(MODELS_DIR, MODEL_NAME),        # Directory to save model checkpoints\n",
    "    evaluation_strategy=\"steps\",                            # Evaluate every N steps\n",
    "    per_device_train_batch_size=8,                          # Batch size for training\n",
    "    per_device_eval_batch_size=8,                           # Batch size for evaluation\n",
    "    gradient_accumulation_steps=2,                          # Gradient accumulation\n",
    "    learning_rate=3e-4,                                     # Learning rate\n",
    "    warmup_steps=500,                                       # Warmup steps for LR scheduler\n",
    "    num_train_epochs=150,                                   # Number of epochs\n",
    "    logging_dir=MODEL_LOGS_DIR,                             # Directory for logging\n",
    "    logging_steps=10,                                       # Log every N steps\n",
    "    save_steps=100,                                         # Save checkpoint every N steps\n",
    "    save_total_limit=2,                                     # Only keep the last 2 checkpoints\n",
    "    fp16=True,                                              # Use mixed precision\n",
    "    dataloader_num_workers=2,                               # Number of workers for DataLoader\n",
    "    load_best_model_at_end=True,                            # Load the best model at the end\n",
    "    metric_for_best_model=\"wer\",                            # Metric to determine best model\n",
    "    greater_is_better=False,                                # Smaller WER is better\n",
    "    seed=42,                                                # Random seed for reproducibility\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                               # Wav2Vec2 model\n",
    "    data_collator=data_collator,               # Data Collator\n",
    "    args=training_args,                        # Training arguments\n",
    "    train_dataset=train_dataset,               # Training dataset\n",
    "    eval_dataset=val_dataset,                  # Validation dataset\n",
    "    processing_class=processor,                # Processor\n",
    "    compute_metrics=compute_metrics,           # WER metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
